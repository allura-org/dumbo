model:
    base_model: HuggingFaceTB/SmolLM2-135M
    liger:
        rope: true
        cross_entropy: false
        fused_linear_cross_entropy: true
        rms_norm: true
        swiglu: true
        

datasets:
    - path: tatsu-lab/alpaca
      type: huggingface_polars # huggingface_polars, huggingface, csv_polars, json_polars, parquet_polars
      data_format: alpaca
      train_format:
        type: jinja_messages
        template: |
            {% for message in messages -%}
            <|im_start|>{{ message.role }}
            {{ message.content }}<|im_end|>
            {% endfor %}

trl:
    trainer_type: sft # sft, dpo, ...
    arguments:
        batch_size: 16
        physical_batch_size: 1 # grad acc is determined by batch size / ngpus / physical batch size
        learning_rate: 1e-4

plugins:
    - transformers
    - liger
    - polars
    - jinja_formatter
    - trl