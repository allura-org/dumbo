model:
    base_model: roneneldan/TinyStories-1M
    tokenizer:
        pad_token: "<|pad|>"
        eos_token: "<|im_end|>"
        bos_token: "<|im_start|>"
    liger:
        rope: true
        cross_entropy: false
        fused_linear_cross_entropy: true
        rms_norm: true
        swiglu: true

transformers:
    trainer:
      arguments:
          batch_size: 16
          physical_batch_size: 1 # grad acc is determined by batch size / ngpus / physical batch size
          learning_rate: 1e-4

datasets:
    - path: tatsu-lab/alpaca
      type: huggingface_polars # huggingface_polars, huggingface, csv_polars, json_polars, parquet_polars
      data_format: alpaca
      train_format:
        type: jinja_messages
        template: |
            {% for message in messages -%}
            <|im_start|>{{ message.role }}
            {{ message.content }}<|im_end|>
            {% endfor %}

plugins:
    - transformers
    - transformers_trainer
    - liger
    - polars
    - jinja_formatter
